{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a266df91-c61c-431e-b4a0-4506cf452f94",
   "metadata": {},
   "source": [
    "# **MODULE 3: Model Personalization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb0573-34b3-444f-ba5b-0ba4d9d77172",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### At the end of this module participants should be able to: \n",
    "           1. Run a parameter space exploration with a single fitting metric​\n",
    "           2. Index and visualise the outputs from the parameter space exploration\n",
    "           3. Explore different fitting metrics and their parameter spaces​\n",
    "           4. Identify an optimal simulation across different fitting metrics\n",
    "           \n",
    "A significant portion of our workflow is adapted from the [Virtual Aging Brain GitHub Repository](https://github.com/ins-amu/virtual_aging_brain) and the [Virtual Ageing Showcase on EBRAINS](https://wiki.ebrains.eu/bin/view/Collabs/sga3-d1-2-showcase-1/).\n",
    "\n",
    "Sample data was obtained from the [Amsterdam PIOP2 Open Dataset](https://nilab-uva.github.io/AOMIC.github.io/) and prepared for simulation using the [TVB-UKBB MRI Processing Pipeline](https://github.com/McIntosh-Lab/tvb-ukbb).\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20697e-1fbe-4b5f-8f3d-51b55c16ad46",
   "metadata": {
    "tags": []
   },
   "source": [
    "### *1. Parameter Space Exploration (PSE) Implementation*\n",
    "\n",
    "REMINDER: As a table, you are competing to identify the parameter combination that produces the **highest FCD variance** for this subject.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d54e1-fef6-4517-96d5-495f091d4aee",
   "metadata": {},
   "source": [
    "#### *Load in the required packages & data for this notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a82c7e-350a-42f2-8310-842312aa591c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import sys, os, time\n",
    "import numpy as np\n",
    "import src\n",
    "from src import viz\n",
    "from src import simulation\n",
    "from src import analysis\n",
    "from tvb.simulator.lab import *\n",
    "from tvb.simulator.backend.nb_mpr import NbMPRBackend\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.signal import savgol_filter\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import matplotlib.patheffects as pe\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Define the function to Load in the SC file - function to be called later\n",
    "def get_connectivity(scaling_factor,sc_path):\n",
    "        SC = np.loadtxt(sc_path)\n",
    "        SC = SC / scaling_factor\n",
    "        conn = connectivity.Connectivity(\n",
    "                weights = SC,\n",
    "                tract_lengths=np.ones_like(SC),\n",
    "                centres = np.zeros(np.shape(SC)[0]),\n",
    "                speed = np.r_[np.Inf]\n",
    "        )\n",
    "        conn.compute_region_labels()\n",
    "\n",
    "        return conn\n",
    "    \n",
    "sub_dir='/tvb_node/tvb/tvb-node-mclab/Session_Materials/data/sub-0001/' #MODIFY\n",
    "scaling_factor=1     #scaling the SC matrix - strength of connections not changed when 1 - no normalization\n",
    "sc_path = os.path.join(sub_dir,'weights.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d4157b-7e50-4a83-96e1-feae60415c4a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### Run the Parameter Space Exploration\n",
    "In the cell below, define the G range, noise range, and the dt value to generate a span of parameter combinations. The code runs a Parameter Space Exploration (PSE) to find the FCD variance for each parameter combination.\n",
    "\n",
    "**For each PSE you run, be sure to proceed to the next cell to generate and save a heatmap. This will save your results in the `Module-3_output_images` folder which can then be added to your slide deck.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176524c-4aaf-4128-b4d8-0ab9595d479f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define parameter ranges below -----------\n",
    "\n",
    "G_start = 1.0\n",
    "G_end = 3.0\n",
    "num_G_points = 3\n",
    "noise_start = 0.01\n",
    "noise_end = 0.1\n",
    "num_noise_points = 3\n",
    "dt = 0.01\n",
    "\n",
    "# Define parameter ranges above -----------\n",
    "\n",
    "\n",
    "\n",
    "sim_len = 10e3\n",
    "FCD_window_size = 5\n",
    "\n",
    "# Create arrays of G and noise values\n",
    "G_values = np.linspace(G_start, G_end, num_G_points)\n",
    "noise_values = np.linspace(noise_start, noise_end, num_noise_points)\n",
    "\n",
    "# Initialize results matrix\n",
    "fcd_var_matrix = np.full((len(noise_values), len(G_values)), np.nan)\n",
    "\n",
    "# Loop through all combinations\n",
    "for i, G in enumerate(G_values):\n",
    "    for j, nsigma in enumerate(noise_values):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Iteration {i*len(noise_values) + j + 1}/{len(G_values)*len(noise_values)}\")\n",
    "        print(f\"G: {G:.4f}, noise: {nsigma:.4f}, dt: {dt}, sim_len: {sim_len}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Set up a simulation object\n",
    "        sim = simulator.Simulator(\n",
    "            connectivity = get_connectivity(scaling_factor, sc_path),\n",
    "            model = models.MontbrioPazoRoxin(\n",
    "                eta   = np.r_[-4.6],\n",
    "                J     = np.r_[14.5],\n",
    "                Delta = np.r_[0.7],\n",
    "                tau   = np.r_[1.],\n",
    "            ),\n",
    "            coupling = coupling.Linear(a=np.r_[G]),\n",
    "            integrator = integrators.HeunStochastic(\n",
    "                dt = dt,\n",
    "                noise = noise.Additive(nsig=np.r_[nsigma, nsigma*2], noise_seed=2)\n",
    "            ),\n",
    "            monitors = [monitors.TemporalAverage(period=0.1)]\n",
    "        ).configure()\n",
    "        \n",
    "        # Run the simulation\n",
    "        runner = NbMPRBackend()\n",
    "        \n",
    "        start_time = time.time() #mark start time \n",
    "        \n",
    "        try:\n",
    "            (tavg_t, tavg_d), = runner.run_sim(sim, simulation_length=sim_len)   #run the sim\n",
    "            simulation_successful = True\n",
    "        except Exception as e:\n",
    "            print(f\"Simulation failed with error: {e}\")\n",
    "            simulation_successful = False\n",
    "            fcd_var_matrix[j, i] = np.nan\n",
    "        \n",
    "        end_time = time.time() #mark end time\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Simulation took: {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        if simulation_successful:\n",
    "            tavg_t *= 10 #convert simulation timepoints to ms\n",
    "            \n",
    "            # Apply the Windkessel model to the simulated data to derive the BOLD time series with TR=2000ms\n",
    "            bold_t, bold_d = simulation.tavg_to_bold(tavg_t, tavg_d, tavg_period=1, \n",
    "                                                     connectivity=sim.connectivity, \n",
    "                                                     svar=0, decimate=2000) \n",
    "            \n",
    "            # Cut the initial transient (e.g., 16 seconds). First ~15 seconds of the balloon model output should be discarded \n",
    "            bold_t = bold_t[8:] \n",
    "            bold_d = bold_d[8:]\n",
    "            \n",
    "            # FCD matrix and FCD variance\n",
    "            FCD, _, _ = analysis.compute_fcd(bold_d[:, 0, :, 0], win_len=FCD_window_size)\n",
    "            \n",
    "            # Calculate FCD variance (only grab the triu from above FCD_window_size diagonal above main to avoid autocorrelations)\n",
    "            fcd_var = np.var(np.triu(FCD, k=FCD_window_size))\n",
    "            \n",
    "            fcd_var_matrix[j, i] = fcd_var\n",
    "            print(f\"FCD_var: {fcd_var:.6f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"All simulations completed!\")\n",
    "print(f\"Total combinations: {len(G_values)} G values × {len(noise_values)} noise values = {len(G_values)*len(noise_values)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474e4b4-85dc-481f-a233-a87557c46606",
   "metadata": {},
   "source": [
    "You may choose to collapse the PSE output above by pressing the bar to the left of the printout.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Visualizing your PSE\n",
    "The cell below creates and saves a heatmap for the PSE you ran in the cell above. The heatmap summarizes the FCDvar for each parameter combination. A printout also reports the combination of G and noise in your search, that yielded a simulation with the highest FCDvar. **Be sure to run the cell below after each PSE you run above**, so that your results are saved in the `Module-3_output_images` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50246bab-1edf-4216-97b2-ca35e66d9a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 1) Find and print maximal FCD_var combination  ---\n",
    "if np.all(np.isnan(fcd_var_matrix)):\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"All entries in fcd_var_matrix are NaN. No maximum can be computed.\")\n",
    "else:\n",
    "    flat_idx = np.nanargmax(fcd_var_matrix)\n",
    "    j_max, i_max = np.unravel_index(flat_idx, fcd_var_matrix.shape)  # j=noise row, i=G col\n",
    "    max_fcdvar = fcd_var_matrix[j_max, i_max]\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \n",
    "    print(\"Max FCD_var (ignoring NaNs):\")\n",
    "    print(f\"  FCD_var = {max_fcdvar:.6e}\")\n",
    "    print(f\"  G       = {G_values[i_max]:.6g}\")\n",
    "    print(f\"  noise   = {noise_values[j_max]:.6g}\")\n",
    "    print(f\"  indices = (noise_row={j_max+1}, G_col={i_max+1})\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \n",
    "# --- 2) Heatmap with NaNs in red ---\n",
    "data = np.array(fcd_var_matrix, dtype=float)\n",
    "\n",
    "# Colormap\n",
    "cmap = plt.cm.viridis.copy()\n",
    "cmap.set_bad(color='red')  # NaNs -> red\n",
    "\n",
    "masked = np.ma.masked_invalid(data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(1.2 * len(G_values) + 3, 0.8 * len(noise_values) + 3))\n",
    "\n",
    "im = ax.imshow(masked, aspect='auto', cmap=cmap, origin='lower')\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(im, ax=ax, shrink=0.9)\n",
    "cbar.set_label(\"FCD_var\")\n",
    "\n",
    "# Axes ticks/labels\n",
    "ax.set_xticks(np.arange(len(G_values)))\n",
    "ax.set_yticks(np.arange(len(noise_values)))\n",
    "ax.set_xticklabels([f\"{g:.3g}\" for g in G_values])\n",
    "ax.set_yticklabels([f\"{n:.3g}\" for n in noise_values])\n",
    "ax.set_xlabel(\"G\")\n",
    "ax.set_ylabel(\"noise (nsigma)\")\n",
    "ax.set_title(\"PSE heatmap (NaNs shown in red)\")\n",
    "\n",
    "ax.set_xticks(np.arange(-0.5, len(G_values), 1), minor=False)\n",
    "ax.set_yticks(np.arange(-0.5, len(noise_values), 1), minor=False)\n",
    "ax.grid(which=\"minor\", linestyle=\"-\", linewidth=1)\n",
    "ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "# --- 3) Annotate each cell with value ---\n",
    "for j in range(len(noise_values)):\n",
    "    for i in range(len(G_values)):\n",
    "        val = data[j, i]\n",
    "        if np.isnan(val):\n",
    "            label = \"NaN\"\n",
    "        else:\n",
    "            label = f\"{val:.3e}\"\n",
    "\n",
    "        txt = ax.text(\n",
    "            i, j, label,\n",
    "            ha=\"center\", va=\"center\",\n",
    "            color=\"black\", fontsize=10\n",
    "        )\n",
    "        txt.set_path_effects([pe.withStroke(linewidth=3, foreground=\"white\")])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"Module-3_output_images/PSE_G-{G_start}-{G_end}-{num_G_points}-points_noise-{noise_start}-{noise_end}-{num_noise_points}-points_dt-{dt}_heatmap.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae12fd",
   "metadata": {},
   "source": [
    "---\n",
    "### *2. Activity Debrief*\n",
    "- What was your **highest FCD variance** value?\n",
    "\n",
    "- What were the **parameter values** (G & Noise) that produced the highest FCD variance value?\n",
    "\n",
    "- What was your **search strategy**?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3760b1-529b-4e47-b174-d60c4a8030a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "###  *3. Alternative metrics of interest*\n",
    "\n",
    "There are multiple model fitting metrics you might want to consider depending on your research goals:\n",
    "- **Maximum FCD variance** → (what we have been using)\n",
    "\n",
    "- **Minimum Kolmogorov–Smirnov (KS) distance** → (a measure of distribution difference; smaller is better) between **Empirical & Simulated FCD**\n",
    "\n",
    "- **Maximum Pearson's correlation** → (higher is better) between **Empirical & Simulated FC**\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Below is example code to compute the FCD variance, FCD KS distance, and FC correlation between the empirical data and a single simulation. Feel free to modify the model parameter values to see how the fitting values change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240ef9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to find metrics on a single sim\n",
    "\n",
    "# Specify the model and model initial parameter values\n",
    "G=1.993      #global coupling\n",
    "nsigma=0.04  #noise variance\n",
    "dt=0.01    #integration step size\n",
    "\n",
    "sim_len=10e3   #length of neural activity to be simulated. With the current setup, 2e3 is 20 seconds.\n",
    "\n",
    "# Set up a simulation object\n",
    "sim = simulator.Simulator(\n",
    "    connectivity = get_connectivity(scaling_factor,sc_path),\n",
    "    model = models.MontbrioPazoRoxin(\n",
    "        eta   = np.r_[-4.6],\n",
    "        J     = np.r_[14.5],\n",
    "        Delta = np.r_[0.7],\n",
    "        tau   = np.r_[1.],\n",
    "    ),\n",
    "    coupling = coupling.Linear(a=np.r_[G]),\n",
    "    integrator = integrators.HeunStochastic(\n",
    "        dt = dt,\n",
    "        noise = noise.Additive(nsig=np.r_[nsigma, nsigma*2], noise_seed=2)\n",
    "    ),\n",
    "    monitors = [monitors.TemporalAverage(period=0.1)]\n",
    ").configure()\n",
    "\n",
    "# Run the simulation\n",
    "runner = NbMPRBackend()\n",
    "\n",
    "start_time = time.time() #mark start time \n",
    "\n",
    "(tavg_t, tavg_d), = runner.run_sim(sim, simulation_length=sim_len)   #run the sim\n",
    "\n",
    "end_time = time.time() #mark end time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Simulation took: {elapsed_time} seconds\")\n",
    "\n",
    "tavg_t *= 10 #convert simulation timepoints to ms\n",
    "\n",
    "\n",
    "#Apply the Windkessel model to the simulated data to derive the BOLD time series with TR=2000ms\n",
    "bold_t, bold_d = simulation.tavg_to_bold(tavg_t, tavg_d, tavg_period=1, connectivity=sim.connectivity, svar=0, decimate=2000) \n",
    "\n",
    "# Cut the initial transient (e.g., 16 seconds). First ~15 seconds of the balloon model output should be discarded \n",
    "bold_t = bold_t[8:] \n",
    "bold_d = bold_d[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb54a4-8ce5-4d30-ad51-ddc4e4c53b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "empFC_path     = f\"data/dld/sub-0001_metric-empFC_length-142.npy\"\n",
    "empFCD_path     = f\"data/dld/sub-0001_metric-empFCD_length-142.npy\"\n",
    "\n",
    "empFC     = np.load(empFC_path, allow_pickle=False)\n",
    "empFCD    = np.load(empFCD_path, allow_pickle=False)\n",
    "\n",
    "def compute_simFC(bold_d_tmp: np.ndarray) -> np.ndarray:\n",
    "    # bold_d_tmp shape expected: (T, 1, N, 1)\n",
    "    x = bold_d_tmp[:, 0, :, 0]  # (T, N)\n",
    "    return np.corrcoef(x, rowvar=False)\n",
    "\n",
    "window = 5\n",
    "\n",
    "# sim FCD\n",
    "FCD, _, _ = analysis.compute_fcd(bold_d[:, 0, :, 0], win_len=window)\n",
    "FCD = np.asarray(FCD)\n",
    "\n",
    "# sim FCDvar\n",
    "FCDvar = float(np.var(np.triu(FCD, k=window)))\n",
    "\n",
    "# sim FC\n",
    "simFC = compute_simFC(bold_d)\n",
    "\n",
    "\n",
    "n_fcd = min(empFCD.shape[0], FCD.shape[0])\n",
    "empFCD_use = empFCD[:n_fcd, :n_fcd]\n",
    "simFCD_use = FCD[:n_fcd, :n_fcd]\n",
    "\n",
    "n_fc = min(empFC.shape[0], simFC.shape[0])\n",
    "empFC_use = empFC[:n_fc, :n_fc]\n",
    "simFC_use = simFC[:n_fc, :n_fc]\n",
    "\n",
    "# KS on upper triangle (k=1), mean-centered\n",
    "iu = np.triu_indices_from(empFCD_use, k=1)\n",
    "empFCD_vals = empFCD_use[iu]\n",
    "simFCD_vals = simFCD_use[iu]\n",
    "FCD_KS, _ = ks_2samp(empFCD_vals - empFCD_vals.mean(), simFCD_vals - simFCD_vals.mean())\n",
    "FCD_KS = float(FCD_KS)\n",
    "\n",
    "# FC correlation on upper triangle flattened (as in your snippet)\n",
    "FC_corr = float(np.corrcoef(np.triu(empFC_use).flatten(), np.triu(simFC_use).flatten())[0, 1])\n",
    "\n",
    "print(\"=== Fitting Metrics Values ===\")\n",
    "print(f\"FC_corr     : {FC_corr:.6f}\")\n",
    "print(f\"FCD_KS      : {FCD_KS:.6f}\")\n",
    "print(f\"FCDvar(sim) : {FCDvar:.6f}\")\n",
    "print(f\"BOLD length : {bold_d.shape[0]} timepoints (after transient cut)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c5744c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### *4. Comparing PSEs for alternative metrics*\n",
    "When performing parameter space explorations (PSE), different model fitting metrics may prioritize different aspects of the data. It is useful to compare PSE results across multiple metrics rather than relying on a single \"best\" fit.\n",
    "\n",
    "<br>\n",
    "\n",
    "For each parameter combination in the grid, we computed **three metrics**:\n",
    "\n",
    "- **Simulated FCD variance**\n",
    "\n",
    "- **KS distance** between the **empirical and simulated FCD**\n",
    "\n",
    "- **Correlation** between the **empirical and simulated FC**\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's load in the parameter spaces for each metric and visually inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4f2bcd-0194-4992-88a9-29fab786645b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pre-completed PSEs across these three metrics \n",
    "\n",
    "# --- Load TSV ---\n",
    "path = \"data/dld/sub-0001_metric-summary_length-142.tsv\"\n",
    "df = pd.read_csv(path, sep=\"\\t\")\n",
    "\n",
    "cols = [\"G\", \"noise\", \"FCDvar\", \"FC_corr\", \"FCD_KS\"]\n",
    "for c in cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df = df.sort_values([\"noise\", \"G\"])\n",
    "\n",
    "\n",
    "# treat FCD_KS==1.0 as missing IF the row looks \"failed\" (other metrics are NaN)\n",
    "sentinel_mask = (df[\"FCD_KS\"] == 1.0) & (df[\"FCDvar\"].isna() | df[\"FC_corr\"].isna())\n",
    "df.loc[sentinel_mask, \"FCD_KS\"] = np.nan\n",
    "\n",
    "\n",
    "print(df[cols].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1dde59-561f-4c6a-ba86-9398ee84fbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def heatmap_from_metric(df, metric, ax):\n",
    "    piv = df.pivot_table(index=\"noise\", columns=\"G\", values=metric, aggfunc=\"mean\")\n",
    "    piv = piv.sort_index().sort_index(axis=1)\n",
    "\n",
    "    data = np.ma.masked_invalid(piv.values)\n",
    "\n",
    "    cmap_name = \"viridis_r\" if metric == \"FCD_KS\" else \"viridis\"\n",
    "    cmap = mpl.cm.get_cmap(cmap_name).copy()\n",
    "    cmap.set_bad(color=\"lightgray\")  # colour for missing cells\n",
    "\n",
    "    im = ax.imshow(data, aspect=\"auto\", origin=\"lower\", cmap=cmap)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel(\"G\")\n",
    "    ax.set_ylabel(\"noise\")\n",
    "    \n",
    "\n",
    "    # Select every Nth tick for x-axis\n",
    "    n_x = len(piv.columns)\n",
    "    step_x = max(1, n_x // 8)  \n",
    "    x_indices = np.arange(0, n_x, step_x)\n",
    "    x_labels = [f\"{piv.columns[i]:g}\" for i in x_indices]\n",
    "    \n",
    "    ax.set_xticks(x_indices)\n",
    "    ax.set_xticklabels(x_labels, rotation=45, ha=\"right\")\n",
    "\n",
    "    # Select every Nth tick for y-axis\n",
    "    n_y = len(piv.index)\n",
    "    step_y = max(1, n_y // 8)  \n",
    "    y_indices = np.arange(0, n_y, step_y)\n",
    "    y_labels = [f\"{piv.index[i]:g}\" for i in y_indices]\n",
    "    \n",
    "    ax.set_yticks(y_indices)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "metrics = [\"FCDvar\", \"FC_corr\", \"FCD_KS\"]\n",
    "    \n",
    "fig, axes = plt.subplots(1, len(metrics), figsize=(5.5 * len(metrics), 4), constrained_layout=True)\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    heatmap_from_metric(df, metric, ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a89fd3",
   "metadata": {},
   "source": [
    "We can compare metrics across the parameter space by vectorizing the PSE outputs and computing the correlations between metric vectors. This allows us to assess whether the metrics yield similar parameter spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29438480",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to vectorize and correlate the parameter spaces\n",
    "\n",
    "piv_fcdvar = df.pivot_table(index=\"noise\", columns=\"G\", values=\"FCDvar\", aggfunc=\"mean\")\n",
    "piv_fc_corr = df.pivot_table(index=\"noise\", columns=\"G\", values=\"FC_corr\", aggfunc=\"mean\")\n",
    "piv_fcd_ks = df.pivot_table(index=\"noise\", columns=\"G\", values=\"FCD_KS\", aggfunc=\"mean\")\n",
    "\n",
    "aligned = (\n",
    "    piv_fcdvar.stack().rename(\"FCDvar\").to_frame()\n",
    "    .join(piv_fc_corr.stack().rename(\"FC_corr\"))\n",
    "    .join(piv_fcd_ks.stack().rename(\"FCD_KS\"))\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "fcdvar_valid = aligned[\"FCDvar\"].to_numpy()\n",
    "fc_corr_valid = aligned[\"FC_corr\"].to_numpy()\n",
    "fcd_ks_valid = aligned[\"FCD_KS\"].to_numpy()\n",
    "\n",
    "corr_fcdvar_fccorr = spearmanr(fcdvar_valid, fc_corr_valid).correlation\n",
    "corr_fcdvar_fcdks = spearmanr(fcdvar_valid, fcd_ks_valid).correlation\n",
    "corr_fccorr_fcdks = spearmanr(fc_corr_valid, fcd_ks_valid).correlation\n",
    "\n",
    "corr_matrix = np.array([\n",
    "    [1.0, corr_fcdvar_fccorr, corr_fcdvar_fcdks],\n",
    "    [corr_fcdvar_fccorr, 1.0, corr_fccorr_fcdks],\n",
    "    [corr_fcdvar_fcdks, corr_fccorr_fcdks, 1.0]\n",
    "])\n",
    "\n",
    "metrics_names = [\"FCDvar\", \"FC_corr\", \"FCD_KS\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\n",
    "\n",
    "axes[0].scatter(fcdvar_valid, fc_corr_valid, alpha=0.6, s=20)\n",
    "axes[0].set_xlabel(\"FCDvar\")\n",
    "axes[0].set_ylabel(\"FC_corr\")\n",
    "axes[0].set_title(f\"r = {corr_fcdvar_fccorr:.3f}\")\n",
    "\n",
    "axes[1].scatter(fcdvar_valid, fcd_ks_valid, alpha=0.6, s=20)\n",
    "axes[1].set_xlabel(\"FCDvar\")\n",
    "axes[1].set_ylabel(\"FCD_KS\")\n",
    "axes[1].set_title(f\"r = {corr_fcdvar_fcdks:.3f}\")\n",
    "\n",
    "axes[2].scatter(fc_corr_valid, fcd_ks_valid, alpha=0.6, s=20)\n",
    "axes[2].set_xlabel(\"FC_corr\")\n",
    "axes[2].set_ylabel(\"FCD_KS\")\n",
    "axes[2].set_title(f\"r = {corr_fccorr_fcdks:.3f}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4), constrained_layout=True)\n",
    "im = ax.imshow(corr_matrix, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(3))\n",
    "ax.set_yticks(range(3))\n",
    "ax.set_xticklabels(metrics_names)\n",
    "ax.set_yticklabels(metrics_names)\n",
    "ax.set_title(\"Spearman Correlation Matrix\")\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        color = \"white\" if abs(corr_matrix[i, j]) > 0.5 else \"black\"\n",
    "        ax.text(j, i, f\"{corr_matrix[i, j]:.2f}\", ha=\"center\", va=\"center\", color=color, fontweight=\"bold\")\n",
    "\n",
    "plt.colorbar(im, ax=ax, label=\"Spearman r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddeb031",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  *5. Selecting an optimal combination of model parameters*\n",
    "\n",
    "In the cell below, we take the output of the parameter space explorations and compute combined rankings across three model fitting metrics:\n",
    "\n",
    "1. Each metric is ranked so that better fits receive higher ranks\n",
    "\n",
    "2. The ranks are summed for each parameter combination\n",
    "\n",
    "This produces a **composite score** to identify the **optimal model parameters (G & Noise)** that reflect overall model fit across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8044d8fc-1660-4930-a0d2-5f79d635e18f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# Load & rank metrics\n",
    "# ---------------------------\n",
    "\n",
    "metric_cols = [\"FCDvar\", \"FC_corr\", \"FCD_KS\"]\n",
    "\n",
    "df_ranked = df.copy()\n",
    "\n",
    "# Handle sentinel values (KS=1.0 when other metrics are NaN)\n",
    "if 'FCD_KS' in df_ranked.columns:\n",
    "    sentinel_mask = (df_ranked[\"FCD_KS\"] == 1.0) & (df_ranked[\"FCDvar\"].isna() | df_ranked[\"FC_corr\"].isna())\n",
    "    df_ranked.loc[sentinel_mask, \"FCD_KS\"] = np.nan\n",
    "\n",
    "# Rank each metric:\n",
    "# - FCDvar: higher is better -> ascending=False (higher rank number for larger values)\n",
    "# - FC_corr: higher is better -> ascending=False\n",
    "# - FCD_KS: lower is better -> ascending=True (higher rank number for smaller values)\n",
    "\n",
    "df_ranked[\"FCDvar_rank\"] = df_ranked[\"FCDvar\"].rank(ascending=False, na_option='keep')\n",
    "df_ranked[\"FC_corr_rank\"] = df_ranked[\"FC_corr\"].rank(ascending=False, na_option='keep')\n",
    "df_ranked[\"FCD_KS_rank\"] = df_ranked[\"FCD_KS\"].rank(ascending=True, na_option='keep')\n",
    "\n",
    "# ---------------------------\n",
    "# Sum ranks\n",
    "# ---------------------------\n",
    "# Sum only when all three ranks are available\n",
    "rank_cols = [\"FCDvar_rank\", \"FC_corr_rank\", \"FCD_KS_rank\"]\n",
    "sum_ranks = df_ranked[rank_cols].sum(axis=1)\n",
    "\n",
    "# Set Sum_of_Ranks to NaN if any rank is NaN\n",
    "df_ranked['Sum_of_Ranks'] = np.where(df_ranked[rank_cols].isna().any(axis=1), np.nan, sum_ranks)\n",
    "\n",
    "# ---------------------------\n",
    "# Get best combined rank (LOWEST sum is best!)\n",
    "# ---------------------------\n",
    "best_idx = df_ranked['Sum_of_Ranks'].idxmin()\n",
    "\n",
    "# Get parameter values for the best combination\n",
    "if not pd.isna(best_idx):\n",
    "    best_noise = df_ranked.loc[best_idx, \"noise\"]\n",
    "    best_G = df_ranked.loc[best_idx, \"G\"]\n",
    "    best_sum = df_ranked.loc[best_idx, \"Sum_of_Ranks\"]\n",
    "    \n",
    "    # Also get the actual metric values for the best point\n",
    "    best_FCDvar = df_ranked.loc[best_idx, \"FCDvar\"]\n",
    "    best_FC_corr = df_ranked.loc[best_idx, \"FC_corr\"]\n",
    "    best_FCD_KS = df_ranked.loc[best_idx, \"FCD_KS\"]\n",
    "    \n",
    "    print(f\"Best combined rank found at:\")\n",
    "    print(f\"  Noise = {best_noise}\")\n",
    "    print(f\"  G = {best_G}\")\n",
    "    print(f\"  Rank sum = {best_sum}\")\n",
    "    print(f\"\\nMetric values at best point:\")\n",
    "    print(f\"  FCDvar = {best_FCDvar:.4f}\")\n",
    "    print(f\"  FC_corr = {best_FC_corr:.4f}\")\n",
    "    print(f\"  FCD_KS = {best_FCD_KS:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No valid combination found (all points have missing metrics)\")\n",
    "\n",
    "    \n",
    "# ---------------------------\n",
    "# Visualize\n",
    "# ---------------------------\n",
    "\n",
    "def plot_rank_sum_heatmap(df_ranked):\n",
    "    \"\"\"Create a heatmap of the rank sum across parameter space\"\"\"\n",
    "    # Pivot to create 2D grid\n",
    "    piv = df_ranked.pivot_table(index=\"noise\", columns=\"G\", values=\"Sum_of_Ranks\", aggfunc=\"mean\")\n",
    "    piv = piv.sort_index().sort_index(axis=1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Create masked array for NaN values\n",
    "    data = np.ma.masked_invalid(piv.values)\n",
    "    \n",
    "    # Use reversed colormap so low (good) ranks are dark, high (bad) are light\n",
    "    cmap = plt.cm.viridis_r.copy()\n",
    "    cmap.set_bad(color='lightgray')\n",
    "    \n",
    "    im = ax.imshow(data, aspect='auto', origin='lower', cmap=cmap)\n",
    "    ax.set_title(\"Sum of Ranks Across Parameter Space\\n(Lower = Better)\")\n",
    "    ax.set_xlabel(\"G\")\n",
    "    ax.set_ylabel(\"noise\")\n",
    "    \n",
    "    # Mark the best point\n",
    "    if not pd.isna(best_idx):\n",
    "        # Find position in pivot table\n",
    "        noise_idx = list(piv.index).index(best_noise)\n",
    "        G_idx = list(piv.columns).index(best_G)\n",
    "        ax.plot(G_idx, noise_idx, 'r*', markersize=15, markeredgecolor='white')\n",
    "    \n",
    "    # Set ticks (every Nth to avoid overcrowding)\n",
    "    n_x = len(piv.columns)\n",
    "    step_x = max(1, n_x // 8)\n",
    "    x_indices = np.arange(0, n_x, step_x)\n",
    "    ax.set_xticks(x_indices)\n",
    "    ax.set_xticklabels([f\"{piv.columns[i]:g}\" for i in x_indices], rotation=45, ha='right')\n",
    "    \n",
    "    n_y = len(piv.index)\n",
    "    step_y = max(1, n_y // 8)\n",
    "    y_indices = np.arange(0, n_y, step_y)\n",
    "    ax.set_yticks(y_indices)\n",
    "    ax.set_yticklabels([f\"{piv.index[i]:g}\" for i in y_indices])\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label=\"Sum of Ranks\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the rank sum heatmap\n",
    "plot_rank_sum_heatmap(df_ranked)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
